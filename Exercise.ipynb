{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59bc44e2",
   "metadata": {},
   "source": [
    "# CSC2611 Lab: Exercise\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64dd9cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from scipy import stats\n",
    "from nltk.corpus import brown\n",
    "from sklearn import decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbeff14",
   "metadata": {},
   "source": [
    "## Load corpus and table data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6435b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_table(file):\n",
    "    word_pairs = []\n",
    "    sim = []\n",
    "    with open(file, 'r') as f:\n",
    "        for line in f:\n",
    "            data = line.split()\n",
    "            word_pairs.append(tuple(data[:2]))\n",
    "            sim.append(float(data[2]))\n",
    "    return word_pairs, sim\n",
    "\n",
    "P, S = load_table('table1.txt')\n",
    "brown_corpus = brown.words()\n",
    "ALL_WORDS = list(set(brown_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feb70c3",
   "metadata": {},
   "source": [
    "## Construct word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fcdb807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 most common words: ['the', 'of', 'and', 'to', 'a']\n",
      "5 least common words: ['tobacco', 'ignore', 'applies', 'relax', 'brass']\n"
     ]
    }
   ],
   "source": [
    "def extract_most_freq(n, corpus):\n",
    "    fdist = nltk.FreqDist(w.lower() for w in corpus if w.isalnum())\n",
    "    most_common = [w for (w, _) in fdist.most_common(n)]\n",
    "    return most_common\n",
    "\n",
    "W = extract_most_freq(5000, brown_corpus)\n",
    "print(f\"5 most common words: {W[:5]}\\n5 least common words: {W[-5:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd1e1a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uniq_words(word_pairs):\n",
    "    uniq_words = []\n",
    "    for pair in word_pairs:\n",
    "        for word in pair:\n",
    "            if word not in uniq_words:\n",
    "                uniq_words.append(word)\n",
    "    return uniq_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c620a876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "|W| = 5028\n"
     ]
    }
   ],
   "source": [
    "def update_W(W, pairs):\n",
    "    new_words = get_uniq_words(pairs)\n",
    "    for word in new_words:\n",
    "        if word not in W and word in ALL_WORDS:\n",
    "            W.append(word)\n",
    "    return W\n",
    "\n",
    "W_= update_W(W, P)\n",
    "print(f\"\\n|W| = {len(W)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd3abffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ('had', 'been') 760\n",
    "def construct_word_context(words, corpus):\n",
    "    n = len(words)\n",
    "    word_idx = {words[i]: i for i in range(n)}\n",
    "    bigrams = list(nltk.bigrams(corpus))\n",
    "    frequency = nltk.FreqDist([(tup[0].lower(), tup[1].lower()) for tup in bigrams])\n",
    "    mat = np.zeros((n,n))\n",
    "    for key, value in frequency.items():\n",
    "        w1, w2 = key[0], key[1]\n",
    "        if w1 in words and w2 in words:\n",
    "            i = word_idx[w2]\n",
    "            j = word_idx[w1]\n",
    "            mat[i, j] = value\n",
    "    return mat, word_idx\n",
    "            \n",
    "M1, W_idx = construct_word_context(W, brown_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5870581e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppmi(mat):\n",
    "    col_sum = np.sum(mat, axis=0)\n",
    "    mat_sum = np.sum(col_sum)\n",
    "    row_sum = np.sum(mat, axis=1)\n",
    "    denom = np.outer(row_sum, col_sum) / mat_sum\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        mat = mat / denom\n",
    "    mat[np.isnan(mat)] = 0.0001\n",
    "    mat[mat==0] = 0.0001\n",
    "    pmi_mat = np.log(mat)\n",
    "    pmi_mat[pmi_mat < 0] = 0.0\n",
    "    return pmi_mat\n",
    "    \n",
    "M1_plus = ppmi(M1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60d08c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_10 = decomposition.PCA(n_components=10)\n",
    "pca_100 = decomposition.PCA(n_components=100)\n",
    "pca_300 = decomposition.PCA(n_components=300)\n",
    "\n",
    "M1_10 = pca_10.fit_transform(M1_plus)\n",
    "M1_100 = pca_100.fit_transform(M1_plus)\n",
    "M1_300 = pca_300.fit_transform(M1_plus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc59f5fc",
   "metadata": {},
   "source": [
    "## Calculate and compare similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33d3acb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_pairs(pairs, sims, words):\n",
    "    new_pairs = []\n",
    "    new_sims = []\n",
    "    for i in range(len(pairs)):\n",
    "        if pairs[i][0] in words and pairs[i][1] in words:\n",
    "            new_pairs.append(pairs[i])\n",
    "            new_sims.append(sims[i])\n",
    "    return new_pairs, new_sims\n",
    "\n",
    "P, S = get_valid_pairs(P, S, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f1b3995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(pairs, mat, w_idx):\n",
    "    vec_pairs = []\n",
    "    for p1, p2 in pairs:\n",
    "        v1 = mat[w_idx[p1]]\n",
    "        v2 = mat[w_idx[p2]]\n",
    "        vec_pairs.append((v1, v2))\n",
    "    return vec_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5acddcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "793deb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_pair_sims(pairs):\n",
    "    sims = []\n",
    "    for v1, v2 in pairs:\n",
    "        sim = cosine_sim(v1, v2)\n",
    "        sims.append(sim)\n",
    "    return sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5d69597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1: 0.1491\n",
      "Model 2: 0.2686\n",
      "Model 3: 0.3311\n"
     ]
    }
   ],
   "source": [
    "def compare_sims(models, pairs, w_idx, similarities):\n",
    "    i = 1\n",
    "    for model in models:\n",
    "        vec_pairs = extract_embeddings(pairs, model, w_idx)\n",
    "        sim_calc = calc_pair_sims(vec_pairs)\n",
    "        r, p = stats.pearsonr(similarities, sim_calc)\n",
    "        print(f'Model {i}: {r:.4f}')\n",
    "        i+=1\n",
    "        \n",
    "r = compare_sims([M1_10, M1_100, M1_300], P, W_idx, S)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
