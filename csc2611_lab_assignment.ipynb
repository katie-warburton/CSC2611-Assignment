{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9857ba9b",
   "metadata": {},
   "source": [
    "# CSC2611 Lab: Word embedding and semantic change\n",
    "\n",
    "Katie Warburton\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9bb2edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from scipy import stats\n",
    "from gensim.models import KeyedVectors\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f07083",
   "metadata": {},
   "source": [
    "## 1. Synchronic word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1995abee",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8089b633",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d1fb56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a Gnsim KeyedVectors model from word vectors\n",
    "def load_LSA_model(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        M1_300, M1_idx = pickle.load(f)\n",
    "    vocab = [key for key in M1_idx.keys()]\n",
    "    model = KeyedVectors(300)\n",
    "    model.add_vectors(vocab, M1_300)\n",
    "    return model, vocab\n",
    "M1_300, vocab = load_LSA_model('m1_300.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73988d9",
   "metadata": {},
   "source": [
    "### Compare similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a75929",
   "metadata": {},
   "source": [
    "#### Load pairs from Table 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0d4d4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in pair data from table 1 of RG65\n",
    "def load_table(file):\n",
    "    word_pairs = []\n",
    "    sim = []\n",
    "    with open(file, 'r') as f:\n",
    "        for line in f:\n",
    "            data = line.split()\n",
    "            word_pairs.append(tuple(data[:2]))\n",
    "            sim.append(float(data[2]))\n",
    "    return word_pairs, sim\n",
    "\n",
    "P, S = load_table('table1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64ce0a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse word pairs and remove ones not in a models vocabulary\n",
    "def get_valid_pairs(pairs, sims, vocab):\n",
    "    new_pairs, new_sims = [], []\n",
    "    for i in range(len(pairs)):\n",
    "        if pairs[i][0] in vocab and pairs[i][1] in vocab:\n",
    "            new_pairs.append(pairs[i])\n",
    "            new_sims.append(sims[i])\n",
    "    return new_pairs, new_sims\n",
    "\n",
    "P, S = get_valid_pairs(P, S, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330e019b",
   "metadata": {},
   "source": [
    "#### Calculate pair similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd6b7a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a word's vector\n",
    "def extract_embeddings(pairs, model):\n",
    "    vec_pairs = []\n",
    "    for w1, w2 in pairs:\n",
    "        vec_pairs.append((model[w1], model[w2]))\n",
    "    return vec_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6015698e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarity\n",
    "def cosine_sim(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80e0cf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cosine similarity between word pairs\n",
    "def calc_pair_sims(pairs):\n",
    "    sims = []\n",
    "    for v1, v2 in pairs:\n",
    "        sim = cosine_sim(v1, v2)\n",
    "        sims.append(sim)\n",
    "    return sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d49f064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute pearson correlation between computed and target similarities \n",
    "def compare_sims(model, pairs, similarities):\n",
    "    vec_pairs = extract_embeddings(pairs, model)\n",
    "    sim_calc = calc_pair_sims(vec_pairs)\n",
    "    r, _ = stats.pearsonr(similarities, sim_calc)\n",
    "    return r\n",
    "\n",
    "r = compare_sims(word2vec, P, S)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416f347f",
   "metadata": {},
   "source": [
    "#### Word2vec vs. human similarity judgement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3f332fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation: 0.7601\n"
     ]
    }
   ],
   "source": [
    "print(f'Pearson correlation: {round(r, 4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66c681a",
   "metadata": {},
   "source": [
    "### Analogy Task\n",
    "\n",
    "#### Load analogy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56304ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data for analogy task\n",
    "def load_analogies(file):\n",
    "    analogies = []\n",
    "    with open(file, 'r') as f:\n",
    "        for line in f:\n",
    "            analogies.append(line.split())\n",
    "    return analogies\n",
    "\n",
    "data = load_analogies('word-test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "caffbd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort analogy tests and remove tests for words not in a vocabulary\n",
    "def get_tests(analogies, vocab):\n",
    "    tests = {}\n",
    "    for line in analogies[1:]:\n",
    "        if line[0] == ':':\n",
    "            key = line[1]\n",
    "            tests[key] = []\n",
    "        else:\n",
    "            # ensure all test words are in the vocabulary \n",
    "            if all(w.lower() in vocab for w in line):\n",
    "                tests[key].append(line)\n",
    "    # remove any test sets with no valid analogies\n",
    "    tests = {key: value for key, value in tests.items() if len(value)>0}\n",
    "    return tests\n",
    "\n",
    "tasks = get_tests(data, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc329581",
   "metadata": {},
   "source": [
    "#### Perform analogy test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a46db6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the missing words in a set of analogies\n",
    "# Return list of 1s and 0s corresponding to correct and incorrect predictions\n",
    "# respectively\n",
    "def predict(analogies, model):\n",
    "    predictions = []\n",
    "    for w1, w2, w3, w4 in analogies:\n",
    "        blank = model.most_similar(positive=[w2, w3], negative=[w1], topn=1)[0][0]\n",
    "        if blank == w4:\n",
    "            predictions.append(1)\n",
    "        else:\n",
    "            predictions.append(0)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9a89a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the analogy test using word2vec\n",
    "def analogy_task_wv(tasks, model):\n",
    "    accuracies = {}\n",
    "    for name in tasks.keys():\n",
    "        analogies = tasks[name]\n",
    "        # 1 = correct prediction, 0 = incorrect\n",
    "        predictions = predict(analogies, model)\n",
    "        acc = sum(predictions)\n",
    "        accuracies[name] = [(acc/len(predictions)), len(predictions)]\n",
    "    return accuracies \n",
    "accuracies_wv = analogy_task_wv(tasks, word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae7dc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the analogy test using LSA model\n",
    "def analogy_task_lsa(tasks, model):\n",
    "    accuracies = {}\n",
    "    for name in tasks.keys():\n",
    "        # model only contains lowercased words\n",
    "        analogies = [[w.lower() for w in analogy] for analogy in tasks[name]]\n",
    "        predictions = predict(analogies, model)\n",
    "        acc = sum(predictions)\n",
    "        accuracies[name] = [(acc/len(predictions)), len(predictions)]\n",
    "    return accuracies \n",
    "accuracies_lsa = analogy_task_lsa(tasks, M1_300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd807ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and display the performance of a model on the analogy task\n",
    "def get_accuracy(accuracies):\n",
    "    sem_acc, sem_count = 0, 0\n",
    "    syn_acc, syn_count = 0, 0\n",
    "    for key, value in accuracies.items():\n",
    "        # all syntactic tasks contain the word 'gram'\n",
    "        if 'gram' in key:\n",
    "            # weight each subtasks accuracy by number of analogies\n",
    "            syn_acc += value[0]*value[1]\n",
    "            syn_count += value[1]\n",
    "        else:\n",
    "            sem_acc += value[0]*value[1]\n",
    "            sem_count += value[1]\n",
    "    sem_acc = round((sem_acc / sem_count)*100, 2)\n",
    "    syn_acc = round((syn_acc / syn_count)*100, 2)\n",
    "    print(f'Syntactic accuracy: {syn_acc}%\\nSemantic accuracy: {sem_acc}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887262ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Word2vec analogy task accuracy\")\n",
    "get_accuracy(accuracies_wv)\n",
    "print(\"\\nLSA model analogy task accuracy\")\n",
    "get_accuracy(accuracies_lsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafe9c2c",
   "metadata": {},
   "source": [
    "## Diachronic word embedding\n",
    "\n",
    "### Prepare diachronic word embeddings\n",
    "\n",
    "Load and format the diachronic word embeddings for analysis. \n",
    "\n",
    "*Note*: There is a small selection of words with zero vectors from 1900 - 1940. For ease of comparison I will remove these vectors as I assume the zero vector means that there was no data for that word in that decade. I ended up with 1994 words for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f56f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load diachronis word embeddings \n",
    "def load_diachronic(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        time_data = pickle.load(f)\n",
    "    w = time_data['w']\n",
    "    d = time_data['d']\n",
    "    E = time_data['E']\n",
    "    return w, d, E\n",
    "\n",
    "w, d, E,  = load_diachronic('data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b92e62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove words from the dataset that do not occur in every decade\n",
    "# ASSUMPTIN: zero vector means word was not found in a decade's data\n",
    "def clean_data(words, vectors):\n",
    "    words_v2 = []\n",
    "    vectors_v2 = []\n",
    "    for i in range(len(vectors)):\n",
    "        flag = False\n",
    "        for vec in vectors[i]:\n",
    "            # if there is a zero vector. \n",
    "            if not np.any(vec):\n",
    "                flag = True\n",
    "        if not flag:\n",
    "            words_v2.append(words[i])\n",
    "            vectors_v2.append(vectors[i])\n",
    "    return words_v2, vectors_v2\n",
    "\n",
    "w, E = clean_data(w, E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e1a329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matrices of vector embedding spaces for each decade\n",
    "def get_vector_space(E, d):\n",
    "    E_by_date = [np.array([E[i][j] for i in range(len(E))]) \n",
    "                 for j in range(len(d))]\n",
    "    return [np.array(vec_list) for vec_list in E_by_date]\n",
    "\n",
    "vec_spaces = get_vector_space(E, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0156c4a9",
   "metadata": {},
   "source": [
    "### Method 1: Nearest neighbours\n",
    "\n",
    "Measuring degree of semantic change by comparing how many nearest neighbours a word has in common with its nearest neighbours from 1900 andn 1990"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb41c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the K nearest neighbours of a word in a vector space\n",
    "def get_nearest_neighbours(word, vector_space, k):\n",
    "    numer = np.dot(vector_space, word)\n",
    "    denom = np.linalg.norm(vector_space, axis=1) * np.linalg.norm(word)\n",
    "    out = np.divide(numer, denom)\n",
    "    # sorted - closest will always be word of interest\n",
    "    idx = set(out.argsort()[-(k+1):-1])\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a8eb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the size of the set intersection between 2 nearest neighbour sets\n",
    "def compare_nn(word_idx, vec_spaces_t1, vec_spaces_t2, k):\n",
    "    word_t1 = vec_spaces_t1[word_idx]\n",
    "    word_t2 = vec_spaces_t2[word_idx]\n",
    "    # each word_idx is unique so don't have to convert back to compare \n",
    "    idx_t1 = get_nearest_neighbours(word_t1, vec_spaces_t1, k)\n",
    "    idx_t2 =  get_nearest_neighbours(word_t2, vec_spaces_t2, k)\n",
    "    overlap = len(idx_t1.intersection(idx_t2))/k\n",
    "    return overlap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616efa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute degree of semantic change according to method 1\n",
    "def semantic_change_1(words, vec_spaces, t1, t2):\n",
    "    overlaps = np.zeros(len(words))\n",
    "    for i in range(len(words)):\n",
    "        overlap = compare_nn(i, vec_spaces[t1], vec_spaces[t2], 50)\n",
    "        overlaps[i] = overlap\n",
    "    return overlaps\n",
    "\n",
    "degree_of_change_1 = semantic_change_1(w, vec_spaces, 0, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5947ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display n most and least changing words\n",
    "def print_n(array, words, n):\n",
    "    sort_arr = list(np.argsort(array)[::-1])\n",
    "    n_max = sort_arr[-n:]\n",
    "    n_min = sort_arr[:n]\n",
    "    max_out = 'Top 20 most changing words'\n",
    "    min_out = 'Top 20 least changing words'\n",
    "    for i in range(n):\n",
    "        max_out += f'\\n{words[n_max[n-i-1]]}'\n",
    "        min_out += f'\\n{words[n_min[i]]}'\n",
    "    print(max_out + \"\\n\\n\" + min_out)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e4196c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print_n(degree_of_change_1, w, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ef6ec2",
   "metadata": {},
   "source": [
    "### Method 2: Matrix Alignment\n",
    "Align matrix from time t1 with matrix from time t2 so the cosine similarity between a word's embeddings in two different time periods can be directly compared. \n",
    "\n",
    "Alignment method based on the webpage \"[FINDING OPTIMAL ROTATION AND TRANSLATION BETWEEN CORRESPONDING 3D POINTS](http://nghiaho.com/?page_id=671)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf15543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the optimal rotation and translation between 2 matrices\n",
    "# Apply the transformation to matrix 1 and return the result\n",
    "def align_matrices(mat1, mat2):\n",
    "    centroid1 = np.mean(mat1, axis=0)\n",
    "    centroid2 = np.mean(mat2, axis=0)\n",
    "    H = np.matmul((mat1-centroid1).T, (mat2-centroid2))\n",
    "    U, _, V = np.linalg.svd(H)\n",
    "    R = np.matmul(U, V)\n",
    "    t = centroid2 - np.dot(R, centroid1)\n",
    "    mat1_trans = np.matmul(mat1, R) + t\n",
    "    return mat1_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b037c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute degree of semantic change according to method 2\n",
    "def semantic_change_2(words, vec_spaces, t1, t2):\n",
    "    sims = np.zeros(len(words))\n",
    "    vec_space_t2 = vec_spaces[t2]\n",
    "    vec_space_t1_align = align_matrices(vec_spaces[t1], vec_space_t2)\n",
    "    for i in range(len(words)):\n",
    "        v1, v2 = vec_space_t1_align[i], vec_space_t2[i] \n",
    "        # higher sim = more changed\n",
    "        sim = cosine_sim(v1, v2)\n",
    "        sims[i] = sim\n",
    "    return sims\n",
    "\n",
    "degree_of_change_2 = semantic_change_2(w, vec_spaces, 0, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ce44b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_n(degree_of_change_2, w, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d6f571",
   "metadata": {},
   "source": [
    "### Method 3: Similarity to nearest neighbours\n",
    "\n",
    "Calculate the cosine similarity between a word and its nearest neighbout at time t1 and again (with the same nearest neighbout) at time t2. Subtract the similarity at t2 from t1 and take the absolute value of the difference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1239c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute degree of semantic change according to method 2\n",
    "def semantic_change_3(words, vec_spaces, t1, t2):\n",
    "    sim_changes = np.zeros(len(words))\n",
    "    vec_space_t1 = vec_spaces[t1]\n",
    "    vec_space_t2 = vec_spaces[t2]\n",
    "    for i in range(len(words)):\n",
    "        v1, v2 = vec_space_t1[i], vec_space_t2[i]\n",
    "        # get nearest neighbour\n",
    "        nn_idx = list(get_nearest_neighbours(v1, vec_space_t1, 1))[0]\n",
    "        nn1, nn2 = vec_space_t1[nn_idx], vec_space_t2[nn_idx]\n",
    "        # compute change in cosine similarity \n",
    "        change = abs(cosine_sim(v1, nn1) - cosine_sim(v2, nn2))\n",
    "        sim_changes[i] = 1 - change\n",
    "    return sim_changes\n",
    "\n",
    "degree_of_change_3 = semantic_change_3(w, vec_spaces, 0, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bd97f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_n(degree_of_change_3, w, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e809ff4",
   "metadata": {},
   "source": [
    "### Method intercorrelations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3ea0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure the intercorrelation among methods\n",
    "def get_pairwise_corr(methods):\n",
    "    n = len(methods)\n",
    "    out = \"\\t\\tMethod 1\\tMethod 2\\tMethod 3\"\n",
    "    for i in range(n):\n",
    "        out += f\"\\nMethod {i+1}\\t\"\n",
    "        for j in range(n):\n",
    "            r, _ = stats.pearsonr(methods[i], methods[j])\n",
    "            out += f\"{r:.4f}\\t\\t\"\n",
    "    print(out)\n",
    "    \n",
    "get_pairwise_corr([degree_of_change_1, degree_of_change_2, degree_of_change_3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a2b6b5",
   "metadata": {},
   "source": [
    "### Evaluate Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0a352c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load semantic_change scores\n",
    "def load_change_scores(file, words):\n",
    "    n = len(words)\n",
    "    scores = np.zeros(n)\n",
    "    j = 0\n",
    "    with open(file, 'rb') as f:\n",
    "        dat = pickle.load(f, encoding='latin1')\n",
    "    for i in range(n):\n",
    "        if words[i] in dat.keys():\n",
    "            # semantic change between 1900 and 1990\n",
    "            scores[i] = dat[words[i]][1900]\n",
    "        else:\n",
    "            j += 1\n",
    "    return scores\n",
    "\n",
    "change_scores = load_change_scores('disps.pkl', w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8367e13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which words have attested scores\n",
    "def get_non_zero(scores):\n",
    "    idx = np.nonzero(scores)\n",
    "    return idx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860d9ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare methods of measuring semantic change to values from\n",
    "# an attested model\n",
    "def compare_methods(scores, methods):\n",
    "    idx = get_non_zero(scores)\n",
    "    scores = scores[idx]\n",
    "    out = \"Method\\tPearson correlation\\tP-value\"\n",
    "    for i in range(len(methods)):\n",
    "        method_scores = methods[i][idx]\n",
    "        r, p = stats.pearsonr(scores, method_scores)\n",
    "        out += f\"\\n{i+1}\\t{round(r, 4)}\\t\\t\\t\"\n",
    "        if p < 0.0001:\n",
    "            out += \"<0.0001\"\n",
    "        else:\n",
    "            out += f\"{round(p, 4)}\"\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6b49c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_methods(change_scores, [degree_of_change_1, degree_of_change_2, degree_of_change_3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4f0652",
   "metadata": {},
   "source": [
    "### Detecting point(s) of semantic change\n",
    "\n",
    "Detecting the point(s) of semantic change for the top 3 changing words according to method 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5bb82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and return the top n most changing words and their indices\n",
    "def max_n(array, words, n):\n",
    "    sort_arr = list(np.argsort(array)[::-1])\n",
    "    n_max = sort_arr[-n:]\n",
    "    n_max.reverse()\n",
    "    top_words = [words[i] for i in n_max]\n",
    "    return top_words, n_max\n",
    "\n",
    "top_3, w_idx = max_n(degree_of_change_2, w, 3)\n",
    "print(f\"The top 3 most changing words are \" + \", \".join(top_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56662c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect the point(s) of semantic change\n",
    "def detect_semantic_change(w_idx, vec_spaces):\n",
    "    changes = []\n",
    "    change_points = []\n",
    "    for i in range(len(vec_spaces)-1):\n",
    "        vec_space_t2 = vec_spaces[-1]\n",
    "        vec_space_t1_align = align_matrices(vec_spaces[i], vec_space_t2)\n",
    "        v1, v2 = vec_space_t1_align[w_idx], vec_space_t2[w_idx] \n",
    "        sim = cosine_sim(v1, v2)\n",
    "        changes.append(sim)\n",
    "    # pairwise rate of change between semantic change scores\n",
    "    pw_rate_of_change = [abs(changes[i+1] - changes[i]) \n",
    "                         for i in range(len(changes)-1)]\n",
    "    # find points where similarity changes significantly\n",
    "    change_points = [i+1 for i in range(len(pw_rate_of_change)) \n",
    "                     if pw_rate_of_change[i] > 0.1]\n",
    "    return changes, change_points\n",
    "changes, change_points =  detect_semantic_change(w.index('objectives'), vec_spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3a335f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a word's meaning along a time series\n",
    "def plot_change_point(changes, dates, word, change_points):\n",
    "    n = len(changes)\n",
    "    fig, ax = plt.subplots() \n",
    "    ax.scatter([i for i in range(n)], changes, color='forestgreen')\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    plt.ylabel('cosine similarity')\n",
    "    plt.xlabel('decade')\n",
    "    plt.title(f\"Semantic change of '{word}' overtime'\")\n",
    "    plt.xticks([i for i in range(n)], dates[:-1])\n",
    "    plt.ylim(0, 1)\n",
    "    ax.vlines(x=change_points, ymin=0, ymax=1, \n",
    "              color='orange', ls=':', label='change point')\n",
    "    plt.legend(bbox_to_anchor=(1.0, 1), loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccd6b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "changes, change_points = detect_semantic_change(w_idx[0], vec_spaces)\n",
    "plot_change_point(changes, d, top_3[0], change_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8200ce9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "changes, change_points = detect_semantic_change(w_idx[1], vec_spaces)\n",
    "plot_change_point(changes, d, top_3[1], change_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530856ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "changes, change_points = detect_semantic_change(w_idx[2], vec_spaces)\n",
    "plot_change_point(changes, d, top_3[2], change_points)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5d391e31",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
